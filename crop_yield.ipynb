{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import tkinter\n",
    "from tkinter import filedialog\n",
    "import numpy as np\n",
    "from tkinter.filedialog import askopenfilename\n",
    "import pandas as pd \n",
    "from tkinter import simpledialog\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Dropout, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow.keras.layers\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "main = tkinter.Tk()\n",
    "main.title(\"Crop Yield Prediction using RNN, Feedforward and LSTM Neural Network\")\n",
    "main.geometry(\"1000x650\")\n",
    "\n",
    "global filename\n",
    "global rnn_acc,lstm_acc, ff_acc\n",
    "global classifier\n",
    "global X, Y, Y1\n",
    "global rainfall_dataset\n",
    "global crop_dataset\n",
    "global le\n",
    "scalerX = StandardScaler()\n",
    "\n",
    "global weight_for_0\n",
    "global weight_for_1\n",
    "\n",
    "def upload():\n",
    "    global filename\n",
    "    global rainfall_dataset\n",
    "    global crop_dataset\n",
    "    global le\n",
    "    filename = filedialog.askdirectory(initialdir = \".\")\n",
    "    rainfall_dataset = pd.read_csv('dataset/district wise rainfall normal.csv')\n",
    "    crop_dataset = pd.read_csv('dataset/Agriculture In India.csv')\n",
    "    crop_dataset.fillna(0, inplace = True)\n",
    "    crop_dataset['Production'] = crop_dataset['Production'].astype(np.int64)\n",
    "    print(crop_dataset.dtypes)\n",
    "    print(crop_dataset['Production'])\n",
    "    text.delete('1.0', END)\n",
    "    text.insert(END,filename+' Loaded\\n\\n')\n",
    "    text.insert(END,str(crop_dataset.head))\n",
    "\n",
    "        \n",
    "\n",
    "def preprocess():\n",
    "    global weight_for_0\n",
    "    global weight_for_1\n",
    "    global crop_dataset\n",
    "    global le\n",
    "    global X, Y\n",
    "    text.delete('1.0', END)\n",
    "    le = LabelEncoder()\n",
    "    crop_dataset['State_Name'] = pd.Series(le.fit_transform(crop_dataset['State_Name']))\n",
    "    crop_dataset['District_Name'] = pd.Series(le.fit_transform(crop_dataset['District_Name']))\n",
    "    crop_dataset['Season'] = pd.Series(le.fit_transform(crop_dataset['Season']))\n",
    "    crop_dataset['Crop'] = pd.Series(le.fit_transform(crop_dataset['Crop']))\n",
    "    crop_datasets = crop_dataset.values\n",
    "    cols = crop_datasets.shape[1]-1\n",
    "    X = crop_datasets[:,0:cols]\n",
    "    Y = crop_datasets[:,cols]\n",
    "    Y = Y.astype('uint8')\n",
    "    avg = np.average(Y)\n",
    "    #avg = avg / 60\n",
    "    Y1 = []\n",
    "    for i in range(len(Y)):\n",
    "        if Y[i] >= avg:\n",
    "            Y1.append(1)\n",
    "        else:\n",
    "            Y1.append(0)\n",
    "    Y = np.asarray(Y1)\n",
    "    Y = Y.astype('uint8')\n",
    "    a,b = np.unique(Y, return_counts=True)\n",
    "    print(str(a)+\" \"+str(b))\n",
    "    Y = to_categorical(Y)\n",
    "    Y = Y.astype('uint8')\n",
    "    counts = np.bincount(Y[:, 0])\n",
    "    weight_for_0 = 1.0 / counts[0]\n",
    "    weight_for_1 = 1.0 / counts[1]\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    scalerX.fit(X)\n",
    "    X = scalerX.transform(X)\n",
    "    text.insert(END,str(X))\n",
    "\n",
    "def runRNN():\n",
    "    global rnn_acc\n",
    "    global X, Y\n",
    "    global classifier\n",
    "    text.delete('1.0', END)\n",
    "    global rnn_acc\n",
    "    global weight_for_0\n",
    "    global weight_for_1\n",
    "    if os.path.exists('model/rnnmodel.json'):\n",
    "        with open('model/rnnmodel.json', \"r\") as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            classifier = model_from_json(loaded_model_json)\n",
    "        classifier.load_weights(\"model/rnnmodel_weights.h5\")\n",
    "        classifier._make_predict_function()   \n",
    "        print(classifier.summary())\n",
    "        f = open('model/rnnhistory.pckl', 'rb')\n",
    "        data = pickle.load(f)\n",
    "        f.close()\n",
    "        accuracy = data[1] * 100\n",
    "        rnn_acc = accuracy\n",
    "        text.insert(END,'RNN Prediction Accuracy : '+str(accuracy)+\"\\n\\n\")\n",
    "    else:\n",
    "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "        rnn = Sequential() #creating RNN model object\n",
    "        rnn.add(Dense(256, input_dim=X.shape[1], activation='relu', kernel_initializer = \"uniform\")) #defining one layer with 256 filters to filter dataset\n",
    "        rnn.add(Dense(128, activation='relu', kernel_initializer = \"uniform\"))#defining another layer to filter dataset with 128 layers\n",
    "        rnn.add(Dense(Y.shape[1], activation='softmax',kernel_initializer = \"uniform\")) #after building model need to predict two classes such as normal or Dyslipidemia disease\n",
    "        rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #while filtering and training dataset need to display accuracy \n",
    "        print(rnn.summary()) #display rnn details\n",
    "        rnn_acc = rnn.fit(X, Y, epochs=2, batch_size=64,class_weight=class_weight) #start building RNN model\n",
    "        values = rnn_acc.history #save each epoch accuracy and loss\n",
    "        values = values['accuracy']\n",
    "        acc = values[1] * 100\n",
    "        rnn_acc = acc;\n",
    "        f = open('model/rnnhistory.pckl', 'wb')\n",
    "        pickle.dump(values, f)\n",
    "        f.close()\n",
    "        text.insert(END,'RNN Prediction Accuracy : '+str(acc)+\"\\n\\n\")\n",
    "        classifier = rnn\n",
    "        classifier.save_weights('model/rnnmodel_weights.h5')\n",
    "        model_json = classifier.to_json()\n",
    "        with open(\"model/rnnmodel.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "def runLSTM():\n",
    "    global lstm_acc\n",
    "    if os.path.exists('model/lstmmodel.json'):\n",
    "        with open('model/lstmmodel.json', \"r\") as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            classifier1 = model_from_json(loaded_model_json)\n",
    "        classifier1.load_weights(\"model/lstmmodel_weights.h5\")\n",
    "        classifier1._make_predict_function()   \n",
    "        print(classifier1.summary())\n",
    "        f = open('model/lstmhistory.pckl', 'rb')\n",
    "        data = pickle.load(f)\n",
    "        f.close()\n",
    "        accuracy = data[1] * 100\n",
    "        lstm_acc = accuracy\n",
    "        text.insert(END,'LSTM Prediction Accuracy : '+str(accuracy)+\"\\n\\n\")\n",
    "    else:\n",
    "        XX = X.reshape((X.shape[0], X.shape[1], 1)) \n",
    "        model = Sequential() #creating LSTM model object\n",
    "        model.add(keras.layers.LSTM(512,input_shape=(X.shape[1], 1))) #defining LSTM layer in sequential object\n",
    "        model.add(Dropout(0.5)) #removing irrelevant dataset features\n",
    "        model.add(Dense(256, activation='relu'))#create another layer\n",
    "        model.add(Dense(Y.shape[1], activation='softmax'))#predict two values as normal or Dyslipidemia disease\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])#calculate accuracy\n",
    "        print(model.summary())\n",
    "        lstm_acc = model.fit(XX, Y, epochs=2, batch_size=64) #start training model\n",
    "        values = lstm_acc.history\n",
    "        values = values['accuracy']\n",
    "        acc = values[1] * 100\n",
    "        lstm_acc = acc\n",
    "        f = open('model/lstmhistory.pckl', 'wb')\n",
    "        pickle.dump(values, f)\n",
    "        f.close()\n",
    "        text.insert(END,'LSTM Prediction Accuracy : '+str(acc)+\"\\n\\n\")\n",
    "        classifier1 = model\n",
    "        classifier1.save_weights('model/lstmmodel_weights.h5')\n",
    "        model_json = classifier1.to_json()\n",
    "        with open(\"model/lstmmodel.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        \n",
    "\n",
    "def runFF():\n",
    "    global ff_acc\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(2, activation='softmax')])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    lstm_acc = model.fit(X, Y, epochs=2, batch_size=64) #start training model\n",
    "    values = lstm_acc.history\n",
    "    values = values['accuracy']\n",
    "    ff_acc = values[1] * 100\n",
    "    text.insert(END,'Feed Forward Neural Network Prediction Accuracy : '+str(ff_acc)+\"\\n\\n\")\n",
    "    \n",
    "\n",
    "def predict():\n",
    "    text.delete('1.0', END)\n",
    "    file = filedialog.askopenfilename(initialdir=\"dataset\")\n",
    "    test = pd.read_csv(file)\n",
    "    test['State_Name'] = pd.Series(le.fit_transform(test['State_Name']))\n",
    "    test['District_Name'] = pd.Series(le.fit_transform(test['District_Name']))\n",
    "    test['Season'] = pd.Series(le.fit_transform(test['Season']))\n",
    "    test['Crop'] = pd.Series(le.fit_transform(test['Crop']))\n",
    "    test = test.values\n",
    "    cols = test.shape[1]\n",
    "    test = test[:,0:cols]\n",
    "    test = scalerX.fit_transform(test)\n",
    "    #test = test.reshape((test.shape[0], test.shape[1], 1)) \n",
    "    print(test.shape)\n",
    "    #test = test[:,0:test.shape[1]] \n",
    "    y_pred = classifier.predict(test)\n",
    "    for i in range(len(test)):\n",
    "        predict = np.argmax(y_pred[i])\n",
    "        print(str(predict))\n",
    "        if predict == 0:\n",
    "            text.insert(END,\"X=%s, Predicted = %s\" % (test[i], 'Predicted Crop Yield will be LESS')+\"\\n\\n\")\n",
    "        else:\n",
    "            text.insert(END,\"X=%s, Predicted = %s\" % (test[i], 'Predicted Crop Yield will be HIGH')+\"\\n\\n\")\n",
    "    \n",
    "def graph():\n",
    "    global rnn_acc,lstm_acc\n",
    "    bars = ['RNN Accuracy','LSTM Accuracy','Feed Forward Accuracy']\n",
    "    height = [rnn_acc,lstm_acc, ff_acc]\n",
    "    y_pos = np.arange(len(bars))\n",
    "    plt.bar(y_pos, height)\n",
    "    plt.xticks(y_pos, bars)\n",
    "    plt.show()\n",
    "\n",
    "def topGraph():\n",
    "    global rainfall_dataset\n",
    "    global crop_dataset\n",
    "    rainfall_dataset = pd.read_csv('dataset/district wise rainfall normal.csv')\n",
    "        \n",
    "    rainfall = rainfall_dataset.groupby(['STATE_UT_NAME'])['ANNUAL'].agg(['sum'])\n",
    "    rainfall = rainfall.sort_values(\"sum\", ascending=False).reset_index()\n",
    "    rainfall = rainfall.loc[0:5]\n",
    "    print(type(rainfall))\n",
    "    rainfall = rainfall.values\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "    for i in range(len(rainfall)):\n",
    "        x1.append(str(rainfall[i,0]))\n",
    "        y1.append(rainfall[i,1])\n",
    "    \n",
    "    rice = pd.read_csv('dataset/Agriculture In India.csv')\n",
    "    rice.fillna(0, inplace = True)\n",
    "    rice['Production'] = rice['Production'].astype(np.int64)\n",
    "    rice = rice.groupby(['State_Name','Crop'])['Production'].agg(['sum'])\n",
    "    rice = rice.sort_values(\"sum\", ascending=False).reset_index()\n",
    "    x2 = []\n",
    "    y2 = []\n",
    "    rice = rice.values\n",
    "    for i in range(len(rice)):\n",
    "        if str(rice[i,1]) == 'Rice':\n",
    "            x2.append(str(rice[i,0]))\n",
    "            y2.append(rice[i,2])\n",
    "            if len(x2) > 5:\n",
    "                break;\n",
    "    x3 = []\n",
    "    y3 = []\n",
    "    for i in range(len(rice)):\n",
    "        if str(rice[i,1]) == 'Coconut':\n",
    "            x3.append(str(rice[i,0]))\n",
    "            y3.append(rice[i,2])\n",
    "            if len(x3) > 5:\n",
    "                break;\n",
    "    x4 = []\n",
    "    y4 = []\n",
    "    for i in range(len(rice)):\n",
    "        if str(rice[i,1]) == 'Sugarcane':\n",
    "            x4.append(str(rice[i,0]))\n",
    "            y4.append(rice[i,2])\n",
    "            if len(x4) > 5:\n",
    "                break;\n",
    "\n",
    "    x5 = []\n",
    "    y5 = []\n",
    "    for i in range(len(rice)):\n",
    "        x5.append(str(rice[i,0]))\n",
    "        y5.append(rice[i,2])\n",
    "        if len(x5) > 5:\n",
    "            break;\n",
    "\n",
    "    fig, ax = plt.subplots(5)\n",
    "    fig.suptitle('Top 6 State Rainfall & Crop Yield')\n",
    "    ax[0].plot(x1,y1.copy())\n",
    "    ax[0].set_title(\"State Vs Rainfall\")\n",
    "    ax[1].plot(x2,y2.copy())\n",
    "    ax[1].set_title(\"Top 6 State Vs Rice Crop Yield\")\n",
    "    ax[2].plot(x3,y3.copy())\n",
    "    ax[2].set_title(\"Top 6 State Vs Coconut Crop Yield\")\n",
    "    ax[3].plot(x4,y4.copy())\n",
    "    ax[3].set_title(\"Top 6 State Vs Sugarcane Crop Yield\")\n",
    "    ax[4].plot(x5,y5.copy())\n",
    "    ax[4].set_title(\"Top 6 State Vs Any Crop Yield\")\n",
    "    plt.show()        \n",
    "\n",
    "    \n",
    "   \n",
    "font = ('times', 15, 'bold')\n",
    "title = Label(main, text='Crop Yield Prediction using RNN, Feedforward and LSTM Neural Network', justify=LEFT)\n",
    "title.config(bg='lavender blush', fg='DarkOrchid1')  \n",
    "title.config(font=font)           \n",
    "title.config(height=3, width=120)       \n",
    "title.place(x=100,y=5)\n",
    "title.pack()\n",
    "\n",
    "font1 = ('times', 12, 'bold')\n",
    "uploadButton = Button(main, text=\"Upload Agriculture Dataset\", command=upload)\n",
    "uploadButton.place(x=10,y=100)\n",
    "uploadButton.config(font=font1)  \n",
    "\n",
    "preprocessButton = Button(main, text=\"Preprocess Dataset\", command=preprocess)\n",
    "preprocessButton.place(x=300,y=100)\n",
    "preprocessButton.config(font=font1)\n",
    "\n",
    "rnnButton = Button(main, text=\"Run RNN Algorithm\", command=runRNN)\n",
    "rnnButton.place(x=480,y=100)\n",
    "rnnButton.config(font=font1)\n",
    "\n",
    "lstmButton = Button(main, text=\"Run LSTM Algorithm\", command=runLSTM)\n",
    "lstmButton.place(x=670,y=100)\n",
    "lstmButton.config(font=font1)\n",
    "\n",
    "ffButton = Button(main, text=\"Run Feedforward Neural Network\", command=runFF)\n",
    "ffButton.place(x=10,y=150)\n",
    "ffButton.config(font=font1)\n",
    "\n",
    "graphButton = Button(main, text=\"Accuracy Comparison Graph\", command=graph)\n",
    "graphButton.place(x=300,y=150)\n",
    "graphButton.config(font=font1)\n",
    "\n",
    "predictButton = Button(main, text=\"Predict Disease using Test Data\", command=predict)\n",
    "predictButton.place(x=10,y=200)\n",
    "predictButton.config(font=font1)\n",
    "\n",
    "topButton = Button(main, text=\"Top 6 Crop Yield Graph\", command=topGraph)\n",
    "topButton.place(x=300,y=200)\n",
    "topButton.config(font=font1)\n",
    "\n",
    "font1 = ('times', 12, 'bold')\n",
    "text=Text(main,height=20,width=160)\n",
    "scroll=Scrollbar(text)\n",
    "text.configure(yscrollcommand=scroll.set)\n",
    "text.place(x=10,y=250)\n",
    "text.config(font=font1) \n",
    "\n",
    "main.config(bg='light coral')\n",
    "main.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State_Name        object\n",
      "District_Name     object\n",
      "Crop_Year          int64\n",
      "Season            object\n",
      "Crop              object\n",
      "Area             float64\n",
      "Production         int64\n",
      "dtype: object\n",
      "                    State_Name District_Name  Crop_Year      Season  \\\n",
      "0  Andaman and Nicobar Islands      NICOBARS       2000      Kharif   \n",
      "1  Andaman and Nicobar Islands      NICOBARS       2000      Kharif   \n",
      "2  Andaman and Nicobar Islands      NICOBARS       2000      Kharif   \n",
      "3  Andaman and Nicobar Islands      NICOBARS       2000  Whole Year   \n",
      "4  Andaman and Nicobar Islands      NICOBARS       2000  Whole Year   \n",
      "\n",
      "                  Crop    Area  Production  \n",
      "0             Arecanut  1254.0        2000  \n",
      "1  Other Kharif pulses     2.0           1  \n",
      "2                 Rice   102.0         321  \n",
      "3               Banana   176.0         641  \n",
      "4            Cashewnut   720.0         165  \n",
      "0           2000\n",
      "1              1\n",
      "2            321\n",
      "3            641\n",
      "4            165\n",
      "           ...  \n",
      "246086       801\n",
      "246087       463\n",
      "246088     16250\n",
      "246089    597899\n",
      "246090        88\n",
      "Name: Production, Length: 246091, dtype: int64\n",
      "(246091, 7)\n",
      "[0 1] [134567 111524]\n",
      "(246091, 6)\n",
      "(246091, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "main = tkinter.Tk()\n",
    "main.title(\"Crop Yield Prediction using RNN, Feedforward and LSTM Neural Network\")\n",
    "main.geometry(\"1000x650\")\n",
    "\n",
    "global filename\n",
    "global rnn_acc,lstm_acc, ff_acc\n",
    "global classifier\n",
    "global X, Y, Y1\n",
    "global rainfall_dataset\n",
    "global crop_dataset\n",
    "global le\n",
    "scalerX = StandardScaler()\n",
    "\n",
    "global weight_for_0\n",
    "global weight_for_1\n",
    "\n",
    "def upload():\n",
    "    global filename\n",
    "    global rainfall_dataset\n",
    "    global crop_dataset\n",
    "    global le\n",
    "    \n",
    "    filename = filedialog.askopenfilename(initialdir=\"dataset\")\n",
    "    rainfall_dataset=pd.read_csv(filename)\n",
    "    file = filedialog.askopenfilename(initialdir=\"dataset\")\n",
    "    crop_dataset=pd.read_csv(file)\n",
    "    \n",
    "    crop_dataset.fillna(0, inplace = True)\n",
    "    crop_dataset['Production'] = crop_dataset['Production'].astype(np.int64)\n",
    "    print(crop_dataset.dtypes)\n",
    "    print(crop_dataset.head())\n",
    "    print(crop_dataset['Production'])\n",
    "    text.delete('1.0', END)\n",
    "    text.insert(END,filename+' Loaded\\n\\n')\n",
    "    text.insert(END,str(crop_dataset.head()))\n",
    "\n",
    "        \n",
    "\n",
    "def preprocess():\n",
    "    global weight_for_0\n",
    "    global weight_for_1\n",
    "    global crop_dataset\n",
    "    global le\n",
    "    global X, Y\n",
    "    text.delete('1.0', END)\n",
    "    le = LabelEncoder()\n",
    "    crop_dataset['State_Name'] = pd.Series(le.fit_transform(crop_dataset['State_Name']))\n",
    "    crop_dataset['District_Name'] = pd.Series(le.fit_transform(crop_dataset['District_Name']))\n",
    "    crop_dataset['Season'] = pd.Series(le.fit_transform(crop_dataset['Season']))\n",
    "    crop_dataset['Crop'] = pd.Series(le.fit_transform(crop_dataset['Crop']))\n",
    "    crop_datasets = crop_dataset.values\n",
    "    cols = crop_datasets.shape[1]-1\n",
    "    print(crop_datasets.shape)\n",
    "    X = crop_datasets[:,0:cols]\n",
    "    Y = crop_datasets[:,cols]\n",
    "    Y = Y.astype('uint8')\n",
    "    avg = np.average(Y)\n",
    "    #avg = avg / 60\n",
    "    Y1 = []\n",
    "    for i in range(len(Y)):\n",
    "        if Y[i] >= avg:\n",
    "            Y1.append(1)\n",
    "        else:\n",
    "            Y1.append(0)\n",
    "    Y = np.asarray(Y1)\n",
    "    Y = Y.astype('uint8')\n",
    "    a,b = np.unique(Y, return_counts=True)\n",
    "    print(str(a)+\" \"+str(b))\n",
    "    Y = to_categorical(Y)\n",
    "    Y = Y.astype('uint8')\n",
    "    counts = np.bincount(Y[:, 0])\n",
    "    weight_for_0 = 1.0 / counts[0]\n",
    "    weight_for_1 = 1.0 / counts[1]\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    scalerX.fit(X)\n",
    "    X = scalerX.transform(X)\n",
    "    text.insert(END,str(X))\n",
    "\n",
    "def runRNN():\n",
    "    global rnn_acc\n",
    "    global X, Y\n",
    "    global classifier\n",
    "    text.delete('1.0', END)\n",
    "    global rnn_acc\n",
    "    global weight_for_0\n",
    "    global weight_for_1\n",
    "    if os.path.exists('model/rnnmodel.json'):\n",
    "        with open('model/rnnmodel.json', \"r\") as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            classifier = model_from_json(loaded_model_json)\n",
    "        classifier.load_weights(\"model/rnnmodel_weights.h5\")\n",
    "        classifier.make_predict_function()   \n",
    "        print(classifier.summary())\n",
    "        f = open('model/rnnhistory.pckl', 'rb')\n",
    "        data = pickle.load(f)\n",
    "        f.close()\n",
    "        accuracy = data[1] * 100\n",
    "        rnn_acc = accuracy\n",
    "        text.insert(END,'RNN Prediction Accuracy : '+str(accuracy)+\"\\n\\n\")\n",
    "    else:\n",
    "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "        rnn = Sequential() #creating RNN model object\n",
    "        rnn.add(SimpleRNN(256, input_dim=X.shape[1], activation='relu', kernel_initializer = \"uniform\")) #defining one layer with 256 filters to filter dataset\n",
    "        rnn.add(SimpleRNN(128, activation='relu', kernel_initializer = \"uniform\"))#defining another layer to filter dataset with 128 layers\n",
    "        rnn.add(Dense(Y.shape[1], activation='sigmoid',kernel_initializer = \"uniform\")) #after building model need to predict two classes such as normal or Dyslipidemia disease\n",
    "        rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #while filtering and training dataset need to display accuracy \n",
    "        print(rnn.summary()) #display rnn details\n",
    "        rnn_acc = rnn.fit(X, Y, epochs=100, batch_size=64,class_weight=class_weight) #start building RNN model\n",
    "        values = rnn_acc.history #save each epoch accuracy and loss\n",
    "        values = values['accuracy']\n",
    "        acc = values[1] * 100\n",
    "        rnn_acc = acc;\n",
    "        f = open('model/rnnhistory.pckl', 'wb')\n",
    "        pickle.dump(values, f)\n",
    "        f.close()\n",
    "        text.insert(END,'RNN Prediction Accuracy : '+str(acc)+\"\\n\\n\")\n",
    "        classifier = rnn\n",
    "        classifier.save_weights('model/rnnmodel_weights.h5')\n",
    "        model_json = classifier.to_json()\n",
    "        with open(\"model/rnnmodel.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "values=0\n",
    "def runLSTM():\n",
    "    global lstm_acc\n",
    "    global classifier\n",
    "    global values\n",
    "    text.delete('1.0',END)\n",
    "    if os.path.exists('model/lstmmodel.json'):\n",
    "        with open('model/lstmmodel.json', \"r\") as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "            classifier= model_from_json(loaded_model_json)\n",
    "        classifier.load_weights(\"model/lstmmodel_weights.h5\")\n",
    "        classifier.make_predict_function()   \n",
    "        print(classifier.summary())\n",
    "        f = open('model/lstmhistory.pckl', 'rb')\n",
    "        data = pickle.load(f)\n",
    "        f.close()\n",
    "        accuracy = data[1] * 100\n",
    "        lstm_acc = accuracy\n",
    "        text.insert(END,'LSTM Prediction Accuracy : '+str(accuracy)+\"\\n\\n\")\n",
    "    else:\n",
    "        XX = np.reshape(X,(X.shape[0], X.shape[1], 1)) \n",
    "        print(XX.shape)\n",
    "        model = Sequential() #creating LSTM model object\n",
    "        model.add(LSTM(512,input_shape=(X.shape[1],1),activation=\"relu\",return_sequences=True)) #defining LSTM layer in sequential object\n",
    "        model.add(Dropout(0.5)) #removing irrelevant dataset features\n",
    "        model.add(LSTM(256, activation='relu',return_sequences=True))#create another layer\n",
    "        model.add(Dense(1, activation='sigmoid'))#predict two values as normal or Dyslipidemia disease\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])#calculate accuracy\n",
    "        print(Y)\n",
    "        #print(model.output_shape)\n",
    "        lstm_acc = model.fit(XX, np.array(l), epochs=100, batch_size=64) #start training model\n",
    "        values = lstm_acc.history\n",
    "        \n",
    "        values1 = values['accuracy']\n",
    "        acc = values1[0] * 100\n",
    "        lstm_acc = acc\n",
    "        #f = open('model/lstmhistory.pckl', 'wb')\n",
    "        #pickle.dump(values, f)\n",
    "        #f.close()\n",
    "        text.insert(END,'LSTM Prediction Accuracy : '+str(acc)+\"\\n\\n\")\n",
    "        classifier = model\n",
    "        classifier.save_weights('model/lstmmodel_weights.h5')\n",
    "        model_json = classifier.to_json()\n",
    "        with open(\"model/lstmmodel.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        \n",
    "\n",
    "def runFF():\n",
    "    global ff_acc\n",
    "    global classifier\n",
    "    text.delete('1.0',END)\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    classifier=model\n",
    "    lstm_acc = model.fit(X, Y, epochs=100, batch_size=64) #start training model\n",
    "    values = lstm_acc.history\n",
    "    values = values['accuracy']\n",
    "    ff_acc = values[1] * 100\n",
    "    text.insert(END,'Feed Forward Neural Network Prediction Accuracy : '+str(ff_acc)+\"\\n\\n\")\n",
    "    \n",
    "\n",
    "def predict():\n",
    "    global classifier\n",
    "    text.delete('1.0', END)\n",
    "    file = filedialog.askopenfilename(initialdir=\"dataset\")\n",
    "    test = pd.read_csv(file)\n",
    "    test['State_Name'] = pd.Series(le.fit_transform(test['State_Name']))\n",
    "    test['District_Name'] = pd.Series(le.fit_transform(test['District_Name']))\n",
    "    test['Season'] = pd.Series(le.fit_transform(test['Season']))\n",
    "    test['Crop'] = pd.Series(le.fit_transform(test['Crop']))\n",
    "    test = test.values\n",
    "    cols = test.shape[1]\n",
    "    test = test[:,0:cols]\n",
    "    test = scalerX.fit_transform(test)\n",
    "    #test = test.reshape((test.shape[0], test.shape[1], 1)) \n",
    "    print(test.shape)\n",
    "    #test = test[:,0:test.shape[1]] \n",
    "    y_pred = classifier.predict(test)\n",
    "    print(y_pred)\n",
    "    for i in range(len(test)):\n",
    "        predict = np.argmax(y_pred[i])\n",
    "        print(str(predict))\n",
    "        if predict == 0:\n",
    "            text.insert(END,\"X=%s, Predicted = %s\" % (test[i], 'Predicted Crop Yield will be LESS')+\"\\n\\n\")\n",
    "        else:\n",
    "            text.insert(END,\"X=%s, Predicted = %s\" % (test[i], 'Predicted Crop Yield will be HIGH')+\"\\n\\n\")\n",
    "\n",
    "def predict1():\n",
    "    global classifier\n",
    "    text.delete('1.0', END)\n",
    "    file = filedialog.askopenfilename(initialdir=\"dataset\")\n",
    "    test = pd.read_csv(file)\n",
    "    test['State_Name'] = pd.Series(le.fit_transform(test['State_Name']))\n",
    "    test['District_Name'] = pd.Series(le.fit_transform(test['District_Name']))\n",
    "    test['Season'] = pd.Series(le.fit_transform(test['Season']))\n",
    "    test['Crop'] = pd.Series(le.fit_transform(test['Crop']))\n",
    "    test = test.values\n",
    "    cols = test.shape[1]\n",
    "    test = test[:,0:cols]\n",
    "    test = scalerX.fit_transform(test)\n",
    "    test = test.reshape((test.shape[0], test.shape[1], 1)) \n",
    "    print(test.shape)\n",
    "    #test = test[:,0:test.shape[1]] \n",
    "    y_pred = classifier.predict(test)\n",
    "    print(y_pred)\n",
    "    for i in range(len(test)):\n",
    "        predict = np.argmax(y_pred[i])\n",
    "        print(str(predict))\n",
    "        if predict == 0:\n",
    "            text.insert(END,\"X=%s, Predicted = %s\" % (test[i], 'Predicted Crop Yield will be LESS')+\"\\n\\n\")\n",
    "        else:\n",
    "            text.insert(END,\"X=%s, Predicted = %s\" % (test[i], 'Predicted Crop Yield will be HIGH')+\"\\n\\n\")\n",
    "            \n",
    "def predict2():\n",
    "    global classifier\n",
    "    text.delete('1.0', END)\n",
    "    file = filedialog.askopenfilename(initialdir=\"dataset\")\n",
    "    test = pd.read_csv(file)\n",
    "    test['State_Name'] = pd.Series(le.fit_transform(test['State_Name']))\n",
    "    test['District_Name'] = pd.Series(le.fit_transform(test['District_Name']))\n",
    "    test['Season'] = pd.Series(le.fit_transform(test['Season']))\n",
    "    test['Crop'] = pd.Series(le.fit_transform(test['Crop']))\n",
    "    test = test.values\n",
    "    cols = test.shape[1]\n",
    "    test = test[:,0:cols]\n",
    "    test = scalerX.fit_transform(test)\n",
    "    #test = test.reshape((test.shape[0], test.shape[1], 1)) \n",
    "    print(test.shape)\n",
    "    #test = test[:,0:test.shape[1]] \n",
    "    y_pred = classifier.predict(test)\n",
    "    print(y_pred)\n",
    "    for i in range(len(test)):\n",
    "        predict = np.argmax(y_pred[i])\n",
    "        print(str(predict))\n",
    "        if predict == 0:\n",
    "            text.insert(END,\"X=%s, Predicted = %s\" % (test[i], 'Predicted Crop Yield will be LESS')+\"\\n\\n\")\n",
    "        else:\n",
    "            text.insert(END,\"X=%s, Predicted = %s\" % (test[i], 'Predicted Crop Yield will be HIGH')+\"\\n\\n\")\n",
    "    \n",
    "def graph():\n",
    "    global rnn_acc,lstm_acc\n",
    "    bars = ['RNN Accuracy','LSTM Accuracy','Feed Forward Accuracy']\n",
    "    height = [rnn_acc,lstm_acc, ff_acc]\n",
    "    y_pos = np.arange(len(bars))\n",
    "    plt.bar(y_pos, height)\n",
    "    plt.xticks(y_pos, bars)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topGraph():\n",
    "    global rainfall_dataset\n",
    "    global crop_dataset\n",
    "    rainfall_dataset = pd.read_csv('dataset/district wise rainfall normal.csv')\n",
    "        \n",
    "    rainfall = rainfall_dataset.groupby(['STATE_UT_NAME'])['ANNUAL'].agg(['sum'])\n",
    "    rainfall = rainfall.sort_values(\"sum\", ascending=False).reset_index()\n",
    "    rainfall = rainfall.loc[0:5]\n",
    "    print(type(rainfall))\n",
    "    rainfall = rainfall.values\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "    for i in range(len(rainfall)):\n",
    "        x1.append(str(rainfall[i,0]))\n",
    "        y1.append(rainfall[i,1])\n",
    "    \n",
    "    rice = pd.read_csv('dataset/Agriculture In India.csv')\n",
    "    rice.fillna(0, inplace = True)\n",
    "    rice['Production'] = rice['Production'].astype(np.int64)\n",
    "    rice = rice.groupby(['State_Name','Crop'])['Production'].agg(['sum'])\n",
    "    rice = rice.sort_values(\"sum\", ascending=False).reset_index()\n",
    "    x2 = []\n",
    "    y2 = []\n",
    "    rice = rice.values\n",
    "    for i in range(len(rice)):\n",
    "        if str(rice[i,1]) == 'Rice':\n",
    "            x2.append(str(rice[i,0]))\n",
    "            y2.append(rice[i,2])\n",
    "            if len(x2) > 5:\n",
    "                break;\n",
    "    x3 = []\n",
    "    y3 = []\n",
    "    for i in range(len(rice)):\n",
    "        if str(rice[i,1]) == 'Coconut':\n",
    "            x3.append(str(rice[i,0]))\n",
    "            y3.append(rice[i,2])\n",
    "            if len(x3) > 5:\n",
    "                break;\n",
    "    x4 = []\n",
    "    y4 = []\n",
    "    for i in range(len(rice)):\n",
    "        if str(rice[i,1]) == 'Sugarcane':\n",
    "            x4.append(str(rice[i,0]))\n",
    "            y4.append(rice[i,2])\n",
    "            if len(x4) > 5:\n",
    "                break;\n",
    "\n",
    "    x5 = []\n",
    "    y5 = []\n",
    "    for i in range(len(rice)):\n",
    "        x5.append(str(rice[i,0]))\n",
    "        y5.append(rice[i,2])\n",
    "        if len(x5) > 5:\n",
    "            break;\n",
    "\n",
    "    \n",
    "    fig1=plt.figure()\n",
    "    axes0=fig1.add_axes([0,0,1.2,0.2])\n",
    "    axes0.plot(x5,y5)\n",
    "    axes0.set_title('top 6 vs any crop yield')\n",
    "    axes1=fig1.add_axes([0,0.35,1.2,0.2])\n",
    "    axes1.plot(x4,y4)\n",
    "    axes1.set_title('top 6 state vs sugar cane')\n",
    "    axes2=fig1.add_axes([0,0.7,1.2,0.2])\n",
    "    axes2.plot(x3,y3)\n",
    "    axes2.set_title('top 6 vs cocount')\n",
    "    axes3=fig1.add_axes([0,1.05,1.2,0.2])\n",
    "    axes3.plot(x2,y2)\n",
    "    axes3.set_title('top 6 vs rice')\n",
    "    axes4=fig1.add_axes([0,1.4,1.2,0.2])\n",
    "    axes4.plot(x1,y1)\n",
    "    axes4.set_title('top 6 rain fall and crop yield')\n",
    "    plt.show()        \n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "font = ('times', 15, 'bold')\n",
    "title = Label(main, text='Crop Yield Prediction using RNN, Feedforward and LSTM Neural Network', justify=LEFT)\n",
    "title.config(bg='lavender blush', fg='DarkOrchid1')  \n",
    "title.config(font=font)           \n",
    "title.config(height=3, width=120)       \n",
    "title.place(x=100,y=5)\n",
    "title.pack()\n",
    "\n",
    "font1 = ('times', 12, 'bold')\n",
    "uploadButton = Button(main, text=\"Upload Agriculture Dataset\", command=upload)\n",
    "uploadButton.place(x=10,y=100)\n",
    "uploadButton.config(font=font1)  \n",
    "\n",
    "preprocessButton = Button(main, text=\"Preprocess Dataset\", command=preprocess)\n",
    "preprocessButton.place(x=300,y=100)\n",
    "preprocessButton.config(font=font1)\n",
    "\n",
    "rnnButton = Button(main, text=\"Run RNN Algorithm\", command=runRNN)\n",
    "rnnButton.place(x=480,y=100)\n",
    "rnnButton.config(font=font1)\n",
    "\n",
    "predictButton = Button(main, text=\"Predict Disease using Test Data2\", command=predict2)\n",
    "predictButton.place(x=550,y=150)\n",
    "predictButton.config(font=font1)\n",
    "\n",
    "lstmButton = Button(main, text=\"Run LSTM Algorithm\", command=runLSTM)\n",
    "lstmButton.place(x=670,y=100)\n",
    "lstmButton.config(font=font1)\n",
    "\n",
    "ffButton = Button(main, text=\"Run Feedforward Neural Network\", command=runFF)\n",
    "ffButton.place(x=10,y=150)\n",
    "ffButton.config(font=font1)\n",
    "\n",
    "graphButton = Button(main, text=\"Accuracy Comparison Graph\", command=graph)\n",
    "graphButton.place(x=300,y=150)\n",
    "graphButton.config(font=font1)\n",
    "\n",
    "predictButton = Button(main, text=\"Predict Disease using Test Data\", command=predict)\n",
    "predictButton.place(x=10,y=200)\n",
    "predictButton.config(font=font1)\n",
    "\n",
    "predictButton = Button(main, text=\"Predict Disease using Test Data1\", command=predict1)\n",
    "predictButton.place(x=300,y=200)\n",
    "predictButton.config(font=font1)\n",
    "\n",
    "topButton = Button(main, text=\"Top 6 Crop Yield Graph\", command=topGraph)\n",
    "topButton.place(x=600,y=200)\n",
    "topButton.config(font=font1)\n",
    "\n",
    "font1 = ('times', 12, 'bold')\n",
    "text=Text(main,height=20,width=160)\n",
    "scroll=Scrollbar(text)\n",
    "text.configure(yscrollcommand=scroll.set)\n",
    "text.place(x=10,y=250)\n",
    "text.config(font=font1) \n",
    "\n",
    "main.config(bg='light coral')\n",
    "main.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import tkinter\n",
    "from tkinter import filedialog\n",
    "import numpy as np\n",
    "from tkinter.filedialog import askopenfilename\n",
    "import pandas as pd \n",
    "from tkinter import simpledialog\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Dropout, Flatten,LSTM\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow.keras.layers\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['dense_1', 'dense_2', 'dense_3']>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=h5py.File(\"model/rnnmodel_weights.h5\",\"r\")\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5py import h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/dense_3\" (1 members)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.get('dense_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5py.File(\"model/lstmmodel_weights.h5\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['dense_4', 'dense_5', 'dropout_1', 'lstm_1']>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/dense_4\" (1 members)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.get(\"dense_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5531856, 0.58845305]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle_in=open(\"model/rnnhistory.pckl\",\"rb\")\n",
    "example_dict=pickle.load(pickle_in)\n",
    "print(example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(Y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       ...,\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       ...,\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       ...,\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in Y:\n",
    "    l.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1], dtype=uint8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5472614765167236]\n"
     ]
    }
   ],
   "source": [
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_tkagg import (FigureCanvasTkAgg, \n",
    "NavigationToolbar2Tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=Figure(figsize=(5,5))\n",
    "plot1=fig.add_subplot(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=[rnn_acc,lstm_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
